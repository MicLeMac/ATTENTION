# Attention

<div align="center">
  <p>
    <strong>Présentation des mécanismes d'attention et des transformers</strong>
  </p>
  <p>
    <img alt="logo ECN" src="https://larevueia.fr/wp-content/uploads/2023/02/Capture-decran-2023-02-04-a-23.02.13.png">
  </p>
</div>

# Utilisation

La présentation sur les mécanismes d'attention est présent sur ce git au format pdf.
Vous pouvez utiliser le NoteBook de ce git pour utiliser un transformer déjà pré-entrainé.

# Sources 

https://arxiv.org/abs/1508.04025
https://arxiv.org/abs/1706.03762
